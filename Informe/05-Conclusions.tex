%!TEX root = main.tex
\section{Conclusions}
\label{sec:conclusions}

This work implemented the gradient-magnitude edge-detection kernel \(G[i] = \sqrt{G_x[i]^2 + G_y[i]^2}\) in modern C++ in four variants: scalar baseline, compiler auto-vectorization, guided (OpenMP SIMD), and explicit (Intel AVX2) vectorization. Performance and vectorization efficiency were analysed with Intel Advisor.

\textbf{[PLACEHOLDER: Summarise the main quantitative result once data are available.]} For instance: the measured execution times are T1 (scalar), T2 (auto), T3 (guided), and T4 (explicit) in the chosen units. The explicit version achieves the best time (T4), and it is observed an improvement of X.X of the explicit version with respect to the implicit (guided) version \textbf{[e.g. ``1.25\(\times\) faster'' or ``about 20\% faster''.]}, and an improvement of X.X with respect to the auto-vectorized version. \textbf{[Adjust these sentences with the actual T1--T4 and computed X.X values.]}

\textbf{[PLACEHOLDER: Interpretation of Advisor results.]} The vectorization percent reported by Intel Advisor is P1 for scalar, P2 for auto, P3 for guided, and P4 for explicit. \textbf{[Add one or two sentences interpreting what this means: e.g. that the compiler auto-vectorised the magnitude loop but not fully the convolution; that the guided pragmas increased vectorisation; that the explicit version shows high (e.g. 100\%) vectorisation in the hot loops.]}

\textbf{[PLACEHOLDER: Brief takeaway.]} In summary, guided and explicit vectorization both improve over the scalar and auto versions for this kernel; the explicit AVX2 implementation gives the highest control and typically the best performance at the cost of portability. Future work could include larger images, other kernels (e.g. full Canny), or comparison with GPU implementations.
